```markdown
# File Storage Strategy

| | |
|---|---|
| **Document ID** | `LWYR-BKND-003` |
| **Document Type** | File Storage Strategy |
| **Category** | Backend |
| **Version** | 1.0 |
| **Date** | 2023-10-27 |
| **Author** | AI Technical Writer |

## 1. Overview

This document outlines the strategy for implementing a secure file storage and retrieval system for the `lawyers` project. Its primary purpose is to address the requirement for a **"secure area for initial document submission"** (Q6), a critical feature for the client onboarding process.

The target users, Non-Resident Indians (NRIs), will need to submit sensitive legal documents as part of the consultation process. Therefore, the solution must prioritize security, confidentiality, access control, and data integrity. This strategy details the chosen technology, architecture, security measures, and data lifecycle management policies to meet these requirements.

The proposed solution uses **pre-signed URLs** to allow clients to upload files directly to a secure, private cloud storage bucket. This approach ensures that sensitive data never passes through our application server, enhancing security and scalability.

## 2. Core Requirements

- **Security:** All documents must be encrypted both in transit (using TLS) and at rest. Access must be strictly controlled.
- **Access Control:** Only the authenticated client who uploaded the document and the advocate (Kalanidhi Sanjeeva Kumar) should be able to access it. Public access must be disabled entirely.
- **Confidentiality:** File metadata and contents are considered confidential legal information and must be protected accordingly.
- **Traceability:** All file uploads must be logged and associated with a specific client and consultation record in the PostgreSQL database.
- **Data Lifecycle:** A clear policy for the retention and eventual deletion of submitted documents must be implemented to minimize data liability.
- **Integration:** The system must integrate seamlessly into the post-booking workflow, providing clients with a clear and simple user interface for uploading files.

## 3. Recommended Technology

### 3.1. Primary Storage Provider: Amazon S3 (Simple Storage Service)

**Amazon S3** is the recommended solution for storing client documents.

**Justification:**
- **Industry Standard:** S3 is a mature, highly durable, and scalable object storage service, widely regarded as the industry standard for application file storage.
- **Robust Security:** Provides extensive security features, including server-side encryption, fine-grained access control via IAM and bucket policies, and comprehensive logging.
- **Cost-Effective:** The pay-as-you-go pricing model is economical for the expected usage patterns.
- **Excellent Integration:** The AWS SDK for Node.js provides robust, well-documented tools for interacting with S3, including generating pre-signed URLs.

### 3.2. Alternative: Vercel Blob

Given the project's deployment on **Vercel**, its native **Vercel Blob** storage is a strong alternative.

**Justification:**
- **Seamless Integration:** Tightly integrated with the Vercel ecosystem, potentially simplifying setup and environment variable management.
- **S3-Compatible API:** Vercel Blob uses an S3-compatible API, meaning the core logic and SDK usage described in this document would require minimal changes.
- **Simplified Operations:** Managed entirely within the Vercel dashboard.

**Decision:** We will proceed with an implementation strategy based on the AWS S3 SDK, as it is compatible with both AWS S3 and Vercel Blob. The final choice can be made during implementation based on operational preference, with AWS S3 being the default for its feature maturity.

## 4. Implementation Strategy

The core of the strategy is to **never let user files touch our application server**. The browser will upload directly to the cloud storage provider using a temporary, secure URL generated by our backend.

### 4.1. File Upload Flow

1.  **Client Authentication:** The client logs into their secure dashboard area after booking a consultation.
2.  **Request Upload URL:** The Next.js frontend makes an authenticated API call to our Node.js backend to request permission to upload a file. The request includes metadata like the intended filename and file type.
3.  **Generate Pre-signed URL:** The backend verifies the user's authentication and authorization. It then uses the AWS SDK to generate a **pre-signed PUT URL**. This URL grants temporary (e.g., 5 minutes) permission to upload a specific file to a unique, non-guessable path in the S3 bucket.
4.  **Direct Upload:** The backend returns the pre-signed URL to the frontend. The frontend then performs a `PUT` request directly to this URL with the file data in the request body.
5.  **Confirm Upload:** Upon a successful upload (HTTP 200 from S3), the frontend makes a second API call to our backend to confirm the upload is complete. This call includes the unique key of the uploaded file.
6.  **Record Metadata:** The backend creates a record in the `Document` table in the PostgreSQL database, linking the file's S3 key to the `User` and their `Consultation`.



### 4.2. Backend Implementation (Node.js/Express)

#### Dependencies
```bash
npm install @aws-sdk/client-s3 @aws-sdk/s3-request-presigner
```

#### Environment Variables
These must be configured securely in Vercel.
```
# AWS Credentials
AWS_ACCESS_KEY_ID="<your-iam-user-access-key>"
AWS_SECRET_ACCESS_KEY="<your-iam-user-secret-key>"
AWS_REGION="ap-south-1" # e.g., Mumbai

# S3 Bucket Name
S3_BUCKET_NAME="lawyers-client-documents-prod"
```

#### API Endpoint: Generate Pre-signed URL (`POST /api/v1/uploads/presigned-url`)
This endpoint is protected and requires a valid JWT.

```javascript
// src/routes/upload.js
import { PutObjectCommand, S3Client } from "@aws-sdk/client-s3";
import { getSignedUrl } from "@aws-sdk/s3-request-presigner";
import { v4 as uuidv4 } from 'uuid';

// ... Inside an authenticated Express route handler ...

const { consultationId, filename, contentType } = req.body;
const userId = req.user.id; // From JWT middleware

// 1. Validate input (file type, size limits etc.)
const ALLOWED_FILE_TYPES = ['application/pdf', 'image/jpeg', 'image/png', 'application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'];
if (!ALLOWED_FILE_TYPES.includes(contentType)) {
  return res.status(400).json({ error: 'Invalid file type.' });
}

// 2. Create a unique key for the S3 object
const uniqueFilename = `${uuidv4()}-${filename}`;
const key = `clients/${userId}/${consultationId}/${uniqueFilename}`;

// 3. Configure S3 Client
const s3Client = new S3Client({
  region: process.env.AWS_REGION,
  credentials: {
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
  },
});

// 4. Create the command for the pre-signed URL
const command = new PutObjectCommand({
  Bucket: process.env.S3_BUCKET_NAME,
  Key: key,
  ContentType: contentType,
});

// 5. Generate the URL (expires in 5 minutes)
const signedUrl = await getSignedUrl(s3Client, command, { expiresIn: 300 });

// 6. Return URL and key to the client
res.json({ signedUrl, key });
```

### 4.3. Database Schema (Prisma)

A `Document` model will be added to `schema.prisma` to track uploads.

```prisma
// schema.prisma

model User {
  id           String        @id @default(cuid())
  // ... other user fields
  documents    Document[]
  consultations Consultation[]
}

model Consultation {
  id          String     @id @default(cuid())
  // ... other consultation fields
  user        User       @relation(fields: [userId], references: [id])
  userId      String
  documents   Document[]
}

model Document {
  id                String       @id @default(cuid())
  s3Key             String       @unique // e.g., clients/user123/consultation456/doc.pdf
  originalFilename  String
  fileType          String       // MIME type
  fileSize          Int          // In bytes
  uploadTimestamp   DateTime     @default(now())

  user              User         @relation(fields: [userId], references: [id])
  userId            String

  consultation      Consultation @relation(fields: [consultationId], references: [id])
  consultationId    String
}
```

### 4.4. File Download Flow (For Advocate)

The advocate will download files via a similar pre-signed URL mechanism, but for `GetObject` operations.

1.  Advocate accesses the admin dashboard and clicks a "Download" link for a client's document.
2.  The frontend calls a secure backend endpoint: `GET /api/v1/documents/:documentId/download-url`.
3.  The backend verifies the advocate's permissions, fetches the document's `s3Key` from the database, and generates a pre-signed **GET URL** with a short expiry.
4.  The backend responds with a 302 redirect to the pre-signed URL, or returns the URL in a JSON payload for the frontend to handle. The browser then downloads the file directly from S3.

## 5. Security & Access Control

### 5.1. Encryption
- **In Transit:** All communication between the client, backend, and S3 will be enforced over HTTPS/TLS.
- **At Rest:** All objects uploaded to the S3 bucket will be encrypted using **Server-Side Encryption with S3-Managed Keys (SSE-S3)**. This uses AES-256 encryption and is enabled by default on new buckets or can be enforced with a bucket policy.

### 5.2. S3 Bucket Policy
The S3 bucket will be configured with a strict policy that **denies all public access**. All access will be managed programmatically via the IAM user and pre-signed URLs.

Example Bucket Policy:
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DenyPublicRead",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::lawyers-client-documents-prod/*",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false"
        }
      }
    },
    {
      "Sid": "DenyIncorrectEncryptionHeader",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::lawyers-client-documents-prod/*",
      "Condition": {
        "StringNotEquals": {
          "s3:x-amz-server-side-encryption": "AES256"
        }
      }
    }
  ]
}
```

### 5.3. CORS Configuration
A CORS (Cross-Origin Resource Sharing) configuration is required on the S3 bucket to allow the website's domain to perform the `PUT` upload request.

Example CORS Configuration:
```xml
<CORSConfiguration>
 <CORSRule>
   <AllowedOrigin>https://www.kalanidhisanjeevakumar.com</AllowedOrigin>
   <AllowedMethod>PUT</AllowedMethod>
   <AllowedMethod>POST</AllowedMethod>
   <AllowedHeader>*</AllowedHeader>
   <MaxAgeSeconds>3000</MaxAgeSeconds>
   <ExposeHeader>ETag</ExposeHeader>
 </CORSRule>
</CORSConfiguration>
```

## 6. Data Lifecycle Management

To comply with data privacy principles and manage storage costs, a data retention policy will be enforced using an **S3 Lifecycle Rule**.

- **Policy:** Documents submitted for an initial consultation will be permanently deleted **90 days** after the `uploadTimestamp`.
- **Implementation:** An S3 Lifecycle Rule will be configured to automatically expire and delete objects prefixed with `clients/`.

**Rationale:** This period provides sufficient time for the advocate to review the documents for the initial consultation and any immediate follow-up. If the client relationship becomes long-term, documents should be manually moved to a permanent, dedicated case management system. This initial upload portal is not intended for long-term archival.

This automated policy ensures data is not retained longer than necessary, reducing liability and administrative overhead.
```